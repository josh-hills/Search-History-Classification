{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "import pickle\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Stop Words\n",
    ">- A stop word is a word that is not important to the meaning of the text. For example, the word \"the\" is a stop word.\n",
    ">- We remove these stop words because words like \"the\" and \"and\" can't help us identify a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()\n",
    "stopWords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def removeStopWords(text):\n",
    "    return [w for w in text if w not in stopWords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Symbols\n",
    ">- We remove symbols because they are not important to the meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSymbols(words):\n",
    "    return [w for w in words if w.isalnum()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization\n",
    ">- Lemmatization is the process of finding the base form of a word.\n",
    ">- For example, \"sing\" is the base form of \"singing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "def lemmatizeStemming(text):\n",
    "    return wordnet_lemmatizer.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This month switch is used to convert the string month given by google to a number\n",
    "monthSwitch = {\n",
    "    'Jan': 1,\n",
    "    'Feb': 2,\n",
    "    'Mar': 3,\n",
    "    'Apr': 4,\n",
    "    'May': 5,\n",
    "    'Jun': 6,\n",
    "    'Jul': 7,\n",
    "    'Aug': 8,\n",
    "    'Sep': 9,\n",
    "    'Oct': 10,\n",
    "    'Nov': 11,\n",
    "    'Dec': 12\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean user searches\n",
    ">- This function takes a path to a user's data and saves a clean version of the data as a pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPkl(userPath):\n",
    "    search = open(userPath + '/' + 'My Activity' + '/' + 'Search' + '/' + 'MyActivity.html', encoding='utf-8')\n",
    "    soup = BeautifulSoup(search, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    searchSplit = text.split('Searched for')\n",
    "    searchSplit = searchSplit[1:]\n",
    "    df = pd.DataFrame(columns=['Search', 'Day', 'Month', 'Year', 'Time'])\n",
    "    \n",
    "    # Loop through each search\n",
    "    for s in searchSplit:\n",
    "        try: \n",
    "            temp = s.split(',')\n",
    "            if len(temp) == 1:\n",
    "                continue\n",
    "            # Get the day, month, year and query of the search\n",
    "            day = temp[0].split(' ')[-3]\n",
    "            month = temp[0].split(' ')[-2]\n",
    "            year = temp[0].split(' ')[-1]\n",
    "            searchQuery = re.findall('[a-zA-Z][^0-9A-Z]*', temp[0])[0]\n",
    "            time = temp[1].split(\" \")[1]\n",
    "            if isinstance(int(day), int) and isinstance(monthSwitch[month], int) and isinstance(int(year), int):\n",
    "                # create a row in the data frame that includes our new query\n",
    "                df = df.append({'Search': searchQuery, 'Day': int(day), 'Month': monthSwitch[month], 'Year': int(year), 'Time': time}, ignore_index=True)\n",
    "        except Exception:\n",
    "            continue\n",
    "    try:\n",
    "        df[['Day', 'Month', 'Year']] = df[['Day', 'Month', 'Year']].astype(int)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Now we perform data clearning, removing numbers, converting to lower case, removing stop words, removing symbols and applyinh lemmatization. Then we save this as a pickle to access later in model.ipynb.\n",
    "    df.dropna(inplace=True)\n",
    "    df['Search'] = df['Search'].str.replace('[0-9]', '')\n",
    "    df['Search'] = df['Search'].str.lower()\n",
    "    df['Search'] = df['Search'].apply(removeStopWords)\n",
    "    df['Search'] = df['Search'].apply(removeSymbols)\n",
    "    df['Search'] = df['Search'].apply(lemmatizeStemming)\n",
    "    df.to_pickle(userPath + '/' + 'search.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run\n",
    ">- Iterate through all of the data within the Data Folder and call createPkl() on each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Josh\\AppData\\Local\\Temp/ipykernel_8896/2871592760.py:33: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Search'] = df['Search'].str.replace('[0-9]', '')\n"
     ]
    }
   ],
   "source": [
    "usersPath = '../Data'\n",
    "for user in os.listdir(usersPath):\n",
    "    createPkl(usersPath+'/'+user)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e526ddeff90a1b6acb572d55cfded43540f1f290e268c586e83fa734f9fafa1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
